import os
import yaml
import random

import torch

from omegaconf import OmegaConf

from lavis.common.registry import registry
from lavis.models import load_model, load_preprocess

os.environ['TOKENIZERS_PARALLELISM'] = 'false'

config = yaml.safe_load(open("classifier/config.yaml"))

random.seed(config["seed"])
torch.manual_seed(config["seed"])

device = torch.device("cuda") if torch.cuda.is_available() else "cpu"

def get_model_and_preprocess(model_dir=None):
    """
        Fetch pre-trained model and preprocess function

        Load the fine-tuned LVLM model from the specified model output directory

        Args:
            model_dir: path to model output directory
        
        Returns:
            Model class and visual preprocessing function (eval)
    """

    name = "blip2_t5_instruct"
    model_type = "flant5xl"

    if model_dir:
        if not os.path.isdir(model_dir): return None, None
        latest_dir = list(sorted(os.listdir(model_dir)))[-1]
        checkpoint_path = os.path.join(model_dir, latest_dir, "checkpoint_best.pth")
        print(f"Using {checkpoint_path}")
        
        model = load_model(name=name,
                        model_type=model_type,
                        is_eval=True,
                        device=device,
                        checkpoint=checkpoint_path,
                        )
    else:
        model = load_model(name=name,
                           model_type=model_type,
                           is_eval=True,
                           device=device)
    
    model_cls = registry.get_model_class(name)
    cfg = OmegaConf.load(model_cls.default_config_path(model_type))

    if cfg is not None:
        preprocess_cfg = cfg.preprocess

        vis_preprocess, _ = load_preprocess(preprocess_cfg)

    assert vis_preprocess, "Visual Preprocessor was not loaded!"

    return model.to(device), vis_preprocess["eval"]

def postprocess(output): 
    """
        Postprocess LVLM output text
        
        Remove brackets and alphabets (if present) from the output text
        To be used particularly for multiple-choice question answer

        Args:
            output: LVLM model output
        
        Raises:
            IndexError: when no numeric values are present in the output text

        Returns:
            Postprocessed ouptut text
    """

    output = output.replace("(", "").replace(")","")
    
    try:
        output = [int(s) for s in output.split(" ") if s.isdigit()][0]
    except IndexError:
        return -1
    
    return output

def generate(model, figures, questions):
    """
        Function to query the LVLM with an input prompt and image

        Args:
            model: LVLM model to query to
            figures: List of images to use in query input
            questions: List of questions to use in query input
        
        Returns:
            answers: uuptut text generated by the model (LVLM)
            scores: text probability score from the model
    """
    samples = {
        "image": figures.to(device),
        "text_input": [f"Question: {q} Answer: " for q in questions],
    }

    answers, scores = model.predict_answers(
            samples=samples,
            inference_method="generate",
            num_beams=5,
            max_len=10,
            min_len=1,
            prompt='',
        )

    return answers, scores